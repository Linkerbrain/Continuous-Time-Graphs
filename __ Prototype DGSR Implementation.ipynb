{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff6a53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import HeteroData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab4208",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae71ecbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>476496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>486432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>482803200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>474422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>475372800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394903</th>\n",
       "      <td>15569</td>\n",
       "      <td>57287</td>\n",
       "      <td>496454400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394904</th>\n",
       "      <td>6783</td>\n",
       "      <td>57287</td>\n",
       "      <td>497232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394905</th>\n",
       "      <td>35430</td>\n",
       "      <td>57288</td>\n",
       "      <td>496800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394906</th>\n",
       "      <td>3542</td>\n",
       "      <td>57288</td>\n",
       "      <td>496886400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394907</th>\n",
       "      <td>49597</td>\n",
       "      <td>57288</td>\n",
       "      <td>496627200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>394908 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            u      i          t\n",
       "0           0      0  476496000\n",
       "1           1      0  486432000\n",
       "2           2      1  482803200\n",
       "3           3      1  474422400\n",
       "4           4      1  475372800\n",
       "...       ...    ...        ...\n",
       "394903  15569  57287  496454400\n",
       "394904   6783  57287  497232000\n",
       "394905  35430  57288  496800000\n",
       "394906   3542  57288  496886400\n",
       "394907  49597  57288  496627200\n",
       "\n",
       "[394908 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('./amazon/Beauty.csv').rename({\"user_id\":\"u\", \"item_id\":\"i\", \"time\":\"t\"}, axis=1)\n",
    "df.head()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b29551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-ordering and fixing time sequences...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12887</td>\n",
       "      <td>473731200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>49582</td>\n",
       "      <td>475372800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>476496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4732</td>\n",
       "      <td>476496001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5760</td>\n",
       "      <td>476496002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394903</th>\n",
       "      <td>52201</td>\n",
       "      <td>57191</td>\n",
       "      <td>493689601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394904</th>\n",
       "      <td>52202</td>\n",
       "      <td>57190</td>\n",
       "      <td>493603200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394905</th>\n",
       "      <td>52202</td>\n",
       "      <td>57191</td>\n",
       "      <td>493603201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394906</th>\n",
       "      <td>52203</td>\n",
       "      <td>57277</td>\n",
       "      <td>490924800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394907</th>\n",
       "      <td>52203</td>\n",
       "      <td>57255</td>\n",
       "      <td>496886400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>394908 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            u      i          t\n",
       "0           0  12887  473731200\n",
       "1           0  49582  475372800\n",
       "2           0      0  476496000\n",
       "3           0   4732  476496001\n",
       "4           0   5760  476496002\n",
       "...       ...    ...        ...\n",
       "394903  52201  57191  493689601\n",
       "394904  52202  57190  493603200\n",
       "394905  52202  57191  493603201\n",
       "394906  52203  57277  490924800\n",
       "394907  52203  57255  496886400\n",
       "\n",
       "[394908 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prep\n",
    "def refine_time(data):\n",
    "    \"\"\"\n",
    "    assures items bought by a user don't have the exact same time\n",
    "    5, 1, 2, 2, 8 -> 1, 2, 3, 5, 8\n",
    "    \"\"\"\n",
    "    \n",
    "    data = data.sort_values(['t'], kind='mergesort')\n",
    "    time_seq = data['t'].values\n",
    "    time_gap = 1\n",
    "    \n",
    "    for i, da in enumerate(time_seq[0:-1]):\n",
    "        if time_seq[i] == time_seq[i+1] or time_seq[i] > time_seq[i+1]:\n",
    "            time_seq[i+1] = time_seq[i+1] + time_gap\n",
    "            time_gap += 1\n",
    "            \n",
    "    data['t'] = time_seq\n",
    "    \n",
    "    return  data\n",
    "\n",
    "def remove_less_than_n_transactions_users(dataf, n):\n",
    "    transactions_per_customer = dataf['u'].value_counts()\n",
    "    \n",
    "    valid_customers = transactions_per_customer[transactions_per_customer>=n].index\n",
    "    \n",
    "    return dataf[dataf['u'].isin(valid_customers)]\n",
    "\n",
    "print(\"Re-ordering and fixing time sequences...\")\n",
    "df = df.groupby('u').apply(refine_time).reset_index(drop=True)\n",
    "df['t'] = df['t'].astype('int64')\n",
    "\n",
    "\n",
    "# This does not work yet since the u's and i's need to be remapped to a continuous range again\n",
    "# min_n = 5\n",
    "# print(f\"Removing users with less than {min_n} transactions\")\n",
    "# df = remove_less_than_n_transactions_users(df, min_n)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8476bcb",
   "metadata": {},
   "source": [
    "# Sample algoritme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8f24b",
   "metadata": {},
   "source": [
    "We sample efficiently by splitting the process up in 3 steps:\n",
    "\n",
    "## Step 1 - Create 'dictionary'\n",
    "\n",
    "Create dictionary of n recent interactions of each user/item\n",
    "\n",
    "made as a big numpy array `u_connections` and `i_connections` met shape (u+1, n) en (i+1, n).\n",
    "\n",
    "index 0 is om aan te geven dat ie geen connecties meer heeft\n",
    "\n",
    "dus stel je wil de items van user [5, 6, 7, 8] weten dan kan je doen u_connections[6, 7, 8, 9]\n",
    "\n",
    "u_connections = [[     0      0      0 ...      0      0      0]\n",
    " [    32     33     34 ...     39     40     41]\n",
    " [    42     43     44 ...      0      0      0]\n",
    " ...\n",
    " [394902 394903      0 ...      0      0      0]\n",
    " [394904 394905      0 ...      0      0      0]\n",
    " [394906 394907      0 ...      0      0      0]]\n",
    " \n",
    " \n",
    "## Step 2 - get_user_network\n",
    "\n",
    "Tweede stap is om het sample algoritme toe te passen, oftwel steeds de items van de users, dan de users van die items, enzovoort. Bij de vorige step hebben we stiekem ook opgeslagen welke transactie nummers van de aankopen er bij horen, en die slaan we op. Dus nu kunnen we de transacties verzamelen die gesampled zijn als edges. En dan eindigen dus we met een subset van de hele dataset, die beperkt is tot die set users en items.\n",
    "\n",
    "\n",
    "## Step 3 - make_graph_object\n",
    "\n",
    "Met deze functie parsen we tot slot die subset van de dataframe naar het dataformaat dat we willen. Hier wordt set gemaakt van users, items en ook de oui en oiu (hoeveelste item van user het is). TODO: Hier moet ook een opsplitsing worden gemaakt voor elke t, oftwel na elk item dat de target user heeft gekocht. Op dit moment is het alleen voor t=-1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c76928",
   "metadata": {},
   "source": [
    "Zorgen/mysteries om nog op te lossen\n",
    "\n",
    "- in het paper halen ze users met > x transacties weg, dat is nu nog lastig omdat het voor de lookup table een continuus range moet zijn\n",
    "\n",
    "- users kunnen meer dan n transacties hebben in de gesamplede ding\n",
    "\n",
    "- hoort het per timestep op nieuw gesampled te worden? wij samplen een keer voor einde van dataset en snijden dan het weg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b3c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created user dictionaries\n",
      "Created item dictionaries\n",
      "Created database of 52204 users and 57289 items\n"
     ]
    }
   ],
   "source": [
    "# create dictionaries\n",
    "\n",
    "# n is max number of recent transactions per node sampled\n",
    "n = 10\n",
    "\n",
    "# -- build User most recent transaction lists --\n",
    "u_connection_list = [np.zeros(n)]\n",
    "u_transaction_list = [np.zeros(n)]\n",
    "\n",
    "for u, us_transactions in df.groupby('u'):\n",
    "    bought = us_transactions['i'].values[-n:]\n",
    "    \n",
    "    zero_padded = np.zeros(n)\n",
    "    zero_padded[:len(bought)] = bought + 1 # offset by 1 for dummy\n",
    "    \n",
    "    u_connection_list.append(zero_padded)\n",
    "    \n",
    "    transaction_idx = us_transactions['i'].index.values[-n:]\n",
    "    \n",
    "    zero_padded_t = np.zeros(n)\n",
    "    zero_padded_t[:len(transaction_idx)] = transaction_idx\n",
    "    \n",
    "    u_transaction_list.append(zero_padded_t)\n",
    "\n",
    "print(\"Created user dictionaries\")\n",
    "\n",
    "# -- build Item most recent transaction lists --\n",
    "i_connection_list = [np.zeros(n)]\n",
    "i_transaction_list = [np.zeros(n)]\n",
    "\n",
    "for i, is_transactions in df.groupby('i'):\n",
    "    bought = is_transactions['u'].values[-n:]\n",
    "    \n",
    "    zero_padded = np.zeros(n)\n",
    "    zero_padded[:len(bought)] = bought + 1 # offset by 1 for dummy\n",
    "    \n",
    "    i_connection_list.append(zero_padded)\n",
    "    \n",
    "    transaction_idx = is_transactions['u'].index.values[-n:]\n",
    "    \n",
    "    zero_padded_t = np.zeros(n)\n",
    "    zero_padded_t[:len(transaction_idx)] = transaction_idx\n",
    "    \n",
    "    i_transaction_list.append(zero_padded_t)\n",
    "\n",
    "print(\"Created item dictionaries\")\n",
    "\n",
    "# -- parse to array --\n",
    "\n",
    "u_connections = np.stack(u_connection_list).astype(np.int32)\n",
    "u_transactions = np.stack(u_transaction_list).astype(np.int32)\n",
    "\n",
    "i_connections = np.stack(i_connection_list).astype(np.int32)\n",
    "i_transactions = np.stack(i_transaction_list).astype(np.int32)\n",
    "\n",
    "print(f\"Created database of {len(u_connections)-1} users and {len(i_connections)-1} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b36006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 52204 users'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_users = df['u'].unique()\n",
    "f\"There are {len(all_users)} users\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e9e4f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user_ids:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([   39,    40,    41, ..., 52093, 52094, 52095])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2077,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'item_ids:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([   17,    43,    84, ..., 57264, 57267, 57277])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(7941,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'trans_ids:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([   339,    340,    341, ..., 394238, 394239, 394240])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(14131,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get user network\n",
    "\n",
    "def get_user_network(index, m=2):\n",
    "    # u_m and i_m are the sets of explored nodes\n",
    "    u_m = np.array([0]) # 0 is dummy\n",
    "    i_m = np.array([0])\n",
    "    \n",
    "    # transactions of sampled nodes\n",
    "    transactions_m = np.array([0])\n",
    "    \n",
    "    # u_temp and i_temp are the sets of unexplored nodes\n",
    "    u_temp = np.array([index+1]) # initialize as the given index\n",
    "    i_temp = u_connections[u_temp] # initialize as its purchases\n",
    "    \n",
    "    # add initialized purchases to transaction base\n",
    "    new_transactions = u_transactions[u_temp].flatten()\n",
    "    transactions_m = np.union1d(transactions_m, new_transactions)\n",
    "        \n",
    "    for j in range(m):\n",
    "        new_users = np.unique(i_connections[i_temp])\n",
    "        u_temp = np.union1d(u_temp, new_users)\n",
    "        \n",
    "        new_transactions = i_transactions[i_temp].flatten()\n",
    "        transactions_m = np.union1d(transactions_m, new_transactions)\n",
    "        \n",
    "        u_temp = np.setdiff1d(u_temp, u_m, assume_unique=True)\n",
    "        u_m = np.union1d(u_m, u_temp)\n",
    "        \n",
    "        if len(u_temp)==0:\n",
    "            break\n",
    "            \n",
    "        new_items = np.unique(u_connections[u_temp])\n",
    "        i_temp = np.union1d(i_temp, new_items)\n",
    "        \n",
    "        new_transactions = u_transactions[u_temp].flatten()\n",
    "        transactions_m = np.union1d(transactions_m, new_transactions)\n",
    "        \n",
    "        i_temp = np.setdiff1d(i_temp, i_m, assume_unique=True)\n",
    "        i_m = np.union1d(i_temp, i_m)\n",
    "        \n",
    "        if len(i_temp)==0:\n",
    "            break\n",
    "    \n",
    "    # [1:] to ignore first element since its dummy 0\n",
    "    # -1 to offset back (it was offset to allow for dummy 0)\n",
    "    return u_m[1:]-1, i_m[1:]-1, transactions_m[1:]\n",
    "\n",
    "user_ids, item_ids, transaction_ids = get_user_network(41)\n",
    "\n",
    "display(\"user_ids:\", user_ids, user_ids.shape, \"item_ids:\", item_ids, item_ids.shape, \"trans_ids:\", transaction_ids, transaction_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4da14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # benchmark run on all users\n",
    "# st = time.time()\n",
    "# selected_nodes = {}\n",
    "# for u in tqdm(all_users[::-1]):\n",
    "#     users, items, trans = get_user_network(u)\n",
    "    \n",
    "#     selected_nodes[u] = trans\n",
    "    \n",
    "# print(f\"{time.time()-st} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd26eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3 make graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0cf647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_oui(df):\n",
    "    \"\"\"\n",
    "    oui = o_u^i = order of u−i interaction\n",
    "    = the position of item i in all items that the u has interacted with\n",
    "    \n",
    "    i is u's item #oui\n",
    "    \"\"\"\n",
    "    return df.groupby(\"u\")[\"t\"].rank(\"first\")\n",
    "\n",
    "def compute_oiu(df):\n",
    "    \"\"\"\n",
    "    oiu = o_i^u = order of i−u interaction\n",
    "    = the position of user u in all users that the i has interacted with\n",
    "    \n",
    "    u is i's buyer #oiu\n",
    "    \"\"\"\n",
    "    return df.groupby(\"i\")[\"t\"].rank(\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b96c00fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  y=17,\n",
       "  \u001b[1mu\u001b[0m={ x=[2030] },\n",
       "  \u001b[1mi\u001b[0m={ x=[7444] },\n",
       "  \u001b[1m(u, bought, i)\u001b[0m={\n",
       "    edge_index=[2030, 7444],\n",
       "    oui=[13074],\n",
       "    oiu=[13074]\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_graph_object(user_index, transaction_ids):\n",
    "    \"\"\"\n",
    "    Makes PyTorch Heterograph not temporal for now\n",
    "    \"\"\"\n",
    "    data = HeteroData()\n",
    "    \n",
    "    sub_df = df.loc[transaction_ids]\n",
    "\n",
    "    # get important transactions\n",
    "    user_sequence_df = sub_df[sub_df['u']==user_index]\n",
    "    final_item = user_sequence_df.iloc[-1]\n",
    "\n",
    "    # remove transactions from the future\n",
    "    sub_df = sub_df[sub_df['t'] < final_item['t']]\n",
    "    \n",
    "    if len(sub_df) < 1:\n",
    "        return data\n",
    "\n",
    "    # make graph\n",
    "    \n",
    "    # remap\n",
    "    mapping_u = {u_id : i for i, u_id in enumerate(sub_df['u'].unique())}\n",
    "    mapping_i = {i_id : i for i, i_id in enumerate(sub_df['i'].unique())}\n",
    "\n",
    "    sub_df['u'] = sub_df['u'].map(mapping_u)\n",
    "    sub_df['i'] = sub_df['i'].map(mapping_i)\n",
    "    \n",
    "    # sort dataframe in same order as pytorch geometric sorts (!!)\n",
    "    sub_df = sub_df.sort_values(['u', 'i'])\n",
    "\n",
    "    # make edge index\n",
    "    users = torch.tensor(sub_df['u'].values)\n",
    "    items = torch.tensor(sub_df['i'].values)\n",
    "\n",
    "    # make edge weights\n",
    "    relative_time = final_item['t'] - sub_df['t'].values\n",
    "    weights = torch.tensor(1 - relative_time / max(relative_time))**3 # SHOULD BE EXPERIMENTED WITH this is unofficial\n",
    "    \n",
    "    # build object\n",
    "    \n",
    "    data['u'].x = torch.tensor(list(mapping_u.keys()))\n",
    "    data['i'].x = torch.tensor(list(mapping_i.keys()))\n",
    "\n",
    "    data['u', 'bought', 'i'].edge_index = torch.sparse_coo_tensor(\n",
    "        torch.stack((users, items)),\n",
    "        weights,\n",
    "        size=(len(mapping_u), len(mapping_i))\n",
    "    ).coalesce()\n",
    "    \n",
    "    data['u', 'bought', 'i'].oui = torch.tensor(compute_oui(sub_df).values, dtype=int) # can maybe be done beforehand TODO\n",
    "    data['u', 'bought', 'i'].oiu = torch.tensor(compute_oiu(sub_df).values, dtype=int)\n",
    "    \n",
    "    data.y = final_item['i']   \n",
    "    \n",
    "    return data\n",
    "\n",
    "user_ids, item_ids, transaction_ids = get_user_network(41)\n",
    "make_graph_object(41, transaction_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e1ad0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412aeecb39914fe7bf02a0e0d7fe9f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.147935152053833 seconds\n"
     ]
    }
   ],
   "source": [
    "# benchmark\n",
    "\n",
    "np.random.shuffle(all_users)\n",
    "min_graph_size = 100\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "graphs = []\n",
    "\n",
    "failed = 0\n",
    "for u in tqdm(all_users[:250]):\n",
    "    user_ids, item_ids, transaction_ids = get_user_network(u)\n",
    "    \n",
    "    if len(transaction_ids) < min_graph_size:\n",
    "        failed += 1\n",
    "        continue\n",
    "        \n",
    "    graph = make_graph_object(u, transaction_ids)\n",
    "    \n",
    "    del(user_ids)\n",
    "    del(item_ids)\n",
    "    del(transaction_ids)\n",
    "    \n",
    "    if len(graph) > 0:\n",
    "        graphs.append(graph)\n",
    "    \n",
    "print(f\"{time.time()-st} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec06fbc1",
   "metadata": {},
   "source": [
    "# DGSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30c0bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5045e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids, item_ids, transaction_ids = get_user_network(41)\n",
    "graph = make_graph_object(41, transaction_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "40788db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[9.9685e-06, 1.0990e-10, 6.9780e-07,  ..., 3.6283e-07, 9.1665e-05,\n",
       "          6.0042e-10],\n",
       "         [3.2736e-06, 2.7381e-07, 3.1657e-08,  ..., 4.3127e-04, 3.7399e-09,\n",
       "          1.7151e-06],\n",
       "         [5.2343e-05, 2.8206e-07, 1.6428e-09,  ..., 1.2057e-06, 3.1164e-04,\n",
       "          4.6403e-10],\n",
       "         ...,\n",
       "         [1.6133e-08, 1.5432e-07, 7.1708e-11,  ..., 6.7970e-04, 2.8008e-05,\n",
       "          2.0587e-11],\n",
       "         [8.1203e-13, 1.8312e-09, 3.0792e-12,  ..., 3.6410e-07, 5.5587e-12,\n",
       "          2.4694e-08],\n",
       "         [1.7497e-12, 7.9909e-05, 3.0776e-06,  ..., 2.4634e-04, 3.8738e-11,\n",
       "          5.2122e-05]], grad_fn=<SoftmaxBackward0>),\n",
       " torch.Size([632, 1656]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sparse_dense_mul(s, d):\n",
    "    \"\"\"\n",
    "    elementwise multiply sparse and dense matrix of same size\n",
    "    \n",
    "    Parameters:\n",
    "        s: sparse matrix\n",
    "        d: dense matrix\n",
    "    \"\"\"\n",
    "    i = s._indices()\n",
    "    v = s._values()\n",
    "    dv = d[i[0,:], i[1,:]]  # get values from relevant entries of dense matrix\n",
    "    return torch.sparse_coo_tensor(i, v * dv, s.size())\n",
    "\n",
    "\n",
    "def pass_messages_no_possitional(messages, adjacency):\n",
    "    \"\"\"\n",
    "    add messages together based on adjacency matrix\n",
    "    \n",
    "    Parameters:\n",
    "        messages: tensor (i, h)\n",
    "        adjacency: sparse tensor (u, i)\n",
    "    \"\"\"\n",
    "    # parse adjacency matrix\n",
    "    user_per_trans, item_per_trans = adjacency._indices()\n",
    "    alpha = adjacency._values().unsqueeze(-1)\n",
    "    \n",
    "    # prepare output\n",
    "    output = torch.zeros((adjacency.shape[0], messages.shape[1]), dtype=float)\n",
    "    \n",
    "    # add messages\n",
    "    output.index_add_(0, user_per_trans, messages[item_per_trans] * alpha)\n",
    "    \n",
    "    # add embeddings\n",
    "    output.index_add_(0, user_per_trans, alpha)\n",
    "    \n",
    "    # TODO: also scale by edge weight ?\n",
    "    \n",
    "    return output\n",
    "\n",
    "def pass_messages(messages, adjacency, pVui):\n",
    "    \"\"\"\n",
    "    add messages together based on adjacency matrix\n",
    "    \n",
    "    Parameters:\n",
    "        messages: tensor (i, h)\n",
    "        adjacency: sparse tensor (u, i)\n",
    "        pVui: tensor (t, h)\n",
    "    \"\"\"\n",
    "    # parse adjacency matrix\n",
    "    user_per_trans, item_per_trans = adjacency._indices()\n",
    "    alpha = adjacency._values().unsqueeze(-1)\n",
    "    \n",
    "    # prepare output\n",
    "    output = torch.zeros((adjacency.shape[0], messages.shape[1]), dtype=float)\n",
    "    \n",
    "    # add messages\n",
    "    output.index_add_(0, user_per_trans, messages[item_per_trans] * alpha)\n",
    "    \n",
    "    # add embeddings\n",
    "    output.index_add_(0, user_per_trans, pVui * alpha)\n",
    "    \n",
    "    # TODO: also scale by edge weight ?\n",
    "    \n",
    "    return output\n",
    "\n",
    "def relative_order(oui, by_who):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        oui (or oiu) : tensor (t) (the order #'s of each transaction)\n",
    "        by_who : tensor (t) (the one to count the # of neighbours from)\n",
    "    \"\"\"    \n",
    "    # compute amount of transactions of each user\n",
    "    neighbourhood_sizes = torch.bincount(by_who)\n",
    "    neighbourhood_sizes_per_trans = neighbourhood_sizes[by_who]\n",
    "\n",
    "    # relative_order = Neighbourhood_size - order (zodat nieuwste altijd hetzelfde hebben)\n",
    "    rui = torch.clip(neighbourhood_sizes_per_trans, max=n) - torch.clip(oui, max=n)\n",
    "    \n",
    "    return rui\n",
    "\n",
    "def get_last(by_who, what, code):\n",
    "    \n",
    "    # compute amount of transactions of each user\n",
    "    neighbourhood_sizes = torch.bincount(by_who)\n",
    "    \n",
    "    # compute cumulative indices of user\n",
    "    cum_ind = torch.cumsum(neighbourhood_sizes, dim=0) - 1\n",
    "    \n",
    "    # select indices out of preferred transactions\n",
    "    last_indices = torch.index_select(what, 0, cum_ind)\n",
    "    \n",
    "    # get item id's from graph\n",
    "    last_ids = torch.index_select(code, 0, last_indices)\n",
    "    \n",
    "    return last_ids\n",
    "    \n",
    "       \n",
    "class DGRNLayer(nn.Module): # Dynamic Graph Recommendation Network\n",
    "    def __init__(self,\n",
    "                 user_num, item_num,\n",
    "                 hidden_size,\n",
    "                 user_max, item_max\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \"\"\" init \"\"\"\n",
    "        self.user_vocab_num = user_num\n",
    "        self.item_vocab_num = item_num\n",
    "        \n",
    "        self.user_max = user_max\n",
    "        self.item_max = item_max\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.sqrt_d = np.sqrt(self.hidden_size)\n",
    "        \n",
    "        \"\"\" layers \"\"\"        \n",
    "        self.w1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False) # Long Term User\n",
    "        self.w2 = nn.Linear(self.hidden_size, self.hidden_size, bias=False) # Long Term Item\n",
    "        \n",
    "        self.w3 = nn.Linear(self.hidden_size, self.hidden_size, bias=False) # Short Term User\n",
    "        self.w4 = nn.Linear(self.hidden_size, self.hidden_size, bias=False) # Short Term Item\n",
    "        \n",
    "        self.pV = nn.Embedding(self.user_max, self.hidden_size) # user positional embedding\n",
    "        self.pK = nn.Embedding(self.item_max, self.hidden_size) # item positional embedding\n",
    "        \n",
    "        self.last_user_embedding = nn.Embedding(self.user_vocab_num, self.hidden_size)\n",
    "        self.last_item_embedding = nn.Embedding(self.item_vocab_num, self.hidden_size)\n",
    "        \n",
    "    def longterm(self, u_embedded, i_embedded, edge_index, rui, riu):\n",
    "        # --- long term ---\n",
    "        \n",
    "        user_messages = self.w2(u_embedded) # (u, h)\n",
    "        item_messages = self.w1(i_embedded) # (i, h)\n",
    "        \n",
    "        # message similarity\n",
    "        e = (user_messages) @ (item_messages).T # (u, i)\n",
    "        e = sparse_dense_mul(edge_index, e) # (u, i)\n",
    "        \n",
    "        user_per_trans, item_per_trans = edge_index.indices()\n",
    "        \n",
    "        # - users to items -\n",
    "            \n",
    "        # compute positional embeddings\n",
    "        pVui = self.pV(rui)\n",
    "        \n",
    "        # dot product van elke pos embedding met betreffende user\n",
    "        u_at_pVui = torch.einsum('ij, ij->i', user_messages[user_per_trans], pVui)\n",
    "        \n",
    "        # alpha is softmax(wu @ wi.T + wu @ p)\n",
    "        e_ui = torch.sparse_coo_tensor(e._indices(), e._values() + u_at_pVui, e.size())  \n",
    "        alphas = torch.sparse.softmax(e_ui / self.sqrt_d, dim=1) # (u, i)\n",
    "        \n",
    "        \n",
    "        # - items to users -\n",
    "        \n",
    "        # compute positional embeddings\n",
    "        pKiu = self.pK(riu)\n",
    "        \n",
    "        # dot product van elke pos embedding met betreffende user\n",
    "        u_at_pKiu = torch.einsum('ij, ij->i', item_messages[item_per_trans], pKiu)\n",
    "        \n",
    "        # beta is softmax(wi @ wu.T + wi @ p)\n",
    "        e_trans = torch.transpose(e, 0, 1)\n",
    "        e_iu = torch.sparse_coo_tensor(e_trans._indices(), e_trans._values() + u_at_pKiu, e_trans.size())        \n",
    "        betas = torch.sparse.softmax(e_iu / self.sqrt_d, dim=1) # (u, i)\n",
    "        \n",
    "        # pass messages\n",
    "        longterm_hu = pass_messages(item_messages, alphas, pKiu)\n",
    "        longterm_hi = pass_messages(user_messages, betas, pVui)\n",
    "        \n",
    "        return longterm_hu, longterm_hi\n",
    "    \n",
    "    def shortterm(self, u_embedded, i_embedded, edge_index, oui, oiu):\n",
    "        \"\"\" TODO \"\"\"\n",
    "        \n",
    "        # --- short term ---\n",
    "        user_per_trans, item_per_trans = edge_index.indices()\n",
    "        \n",
    "        user_messages = self.w2(u_embedded) # (u, h)\n",
    "        item_messages = self.w1(i_embedded) # (i, h)\n",
    "        \n",
    "        # Get last item\n",
    "        last_item = get_last(user_per_trans, item_per_trans, graph['i'].x)\n",
    "        last_item_embedding = self.last_item_embedding(last_item)\n",
    "        last_item = self.w3(last_item_embedding)\n",
    "        \n",
    "        # Get last user from items\n",
    "        last_user = get_last(item_per_trans, user_per_trans, graph['u'].x)\n",
    "        last_user_embedding = self.last_user_embedding(last_user)\n",
    "        last_user = self.w4(last_user_embedding)\n",
    "        \n",
    "        # message similarity alpha\n",
    "        a = (last_item) @ (item_messages).T # (u, i)\n",
    "        a = sparse_dense_mul(edge_index, a) # (u, i)\n",
    "        \n",
    "        # message similarity beta\n",
    "        b = (last_user) @ (item_messages).T # (u, i)\n",
    "        b = sparse_dense_mul(edge_index, b) # (u, i)\n",
    "        \n",
    "        \n",
    "        # compute alphas\n",
    "        a = torch.sparse_coo_tensor(a._indices(), a._values(), a.size())        \n",
    "        alphas = torch.sparse.softmax(a / self.sqrt_d, dim=1) # (u, i)\n",
    "        \n",
    "        \n",
    "        # compute betas\n",
    "        b_trans = torch.transpose(b, 0, 1)\n",
    "        b = torch.sparse_coo_tensor(b_trans._indices(), b_trans._values(), b_trans.size())  \n",
    "        betas = torch.sparse.softmax(b / self.sqrt_d, dim=1) # (u, i)\n",
    "\n",
    "        \n",
    "        # pass messages\n",
    "        shortterm_hu = pass_messages_no_possitional(item_messages, alphas)\n",
    "        shortterm_hi = pass_messages_no_possitional(user_messages, betas)\n",
    "\n",
    "        \n",
    "        return shortterm_hu, shortterm_hi\n",
    "        \n",
    "        \n",
    "    def forward(self, u_emb, i_emb, edge_index, rui, riu, oui, oiu):\n",
    "        # propagate information\n",
    "        # longterm\n",
    "        hLu, hLi = self.longterm(u_emb, i_emb, edge_index, rui, riu)\n",
    "        \n",
    "        # shortterm\n",
    "        hSu, hSi = self.shortterm(u_emb, i_emb, edge_index, oui, oiu)\n",
    "        \n",
    "        return hLu, hSu, hLi, hSi\n",
    "        \n",
    "class DGSRNetwork(nn.Module): # Dynamic Graph Recommendation Network\n",
    "    def __init__(self,\n",
    "                 user_num, item_num,\n",
    "                 hidden_size,\n",
    "                 user_max, item_max,\n",
    "                 num_DGRN_layers\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \"\"\" init \"\"\"\n",
    "        self.user_vocab_num = user_num\n",
    "        self.item_vocab_num = item_num\n",
    "        \n",
    "        self.user_max = user_max\n",
    "        self.item_max = item_max\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.sqrt_d = np.sqrt(self.hidden_size)\n",
    "        \n",
    "        \"\"\" layers \"\"\"\n",
    "        # embedding\n",
    "        self.user_embedding = nn.Embedding(self.user_vocab_num, self.hidden_size)\n",
    "        self.item_embedding = nn.Embedding(self.item_vocab_num, self.hidden_size)\n",
    "        \n",
    "        # propogation\n",
    "        self.DGSRLayers = nn.ModuleList()\n",
    "        for _ in range(num_DGRN_layers):\n",
    "            self.DGSRLayers.append(DGRNLayer(user_num, item_num, hidden_size, user_max, item_max))\n",
    "        \n",
    "        # node updating\n",
    "        self.w3 = nn.Linear(self.hidden_size*3, self.hidden_size, bias=False)\n",
    "        self.w4 = nn.Linear(self.hidden_size*3, self.hidden_size, bias=False)\n",
    "        \n",
    "        # recommendation\n",
    "        self.wP = nn.Linear(self.hidden_size, self.hidden_size*(num_DGRN_layers+1), bias=False)\n",
    "        \n",
    "    def forward(self, graph):\n",
    "        \n",
    "        # embedding\n",
    "        hu = self.user_embedding(graph['u'].x) # (u, h)\n",
    "        hi = self.item_embedding(graph['i'].x) # (i, h)\n",
    "        \n",
    "        # parse graph\n",
    "        edges = graph['u', 'bought', 'i'].edge_index\n",
    "        oui = graph['u', 'bought', 'i'].oui\n",
    "        oiu = graph['u', 'bought', 'i'].oiu\n",
    "        \n",
    "        user_per_trans, item_per_trans = edges.indices()\n",
    "        \n",
    "        rui = relative_order(oui, user_per_trans)\n",
    "        riu = relative_order(oiu, item_per_trans)\n",
    "        \n",
    "        # propogation\n",
    "        \n",
    "        # iterate over Dynamic Graph Sequential Recommendation Layers\n",
    "        hu_list = [hu]\n",
    "        hi_list = [hi]\n",
    "        for DGSR in self.DGSRLayers:\n",
    "            hLu, hSu, hLi, hSi = DGSR(hu, hi, edges, rui, riu, oui, oiu)\n",
    "            \n",
    "            # concatenate information\n",
    "            hu_concat = torch.hstack((hLu, hSu, hu)).float()\n",
    "            hi_concat = torch.hstack((hLi, hSi, hi)).float()\n",
    "            \n",
    "            # make new embedding\n",
    "            hu = torch.tanh(self.w3(hu_concat))\n",
    "            hi = torch.tanh(self.w4(hi_concat))\n",
    "            \n",
    "            # save user embedding at every timestep\n",
    "            hu_list.append(hu)\n",
    "            hi_list.append(hi)\n",
    "        \n",
    "        # recommendation\n",
    "        prediction_user_embedding = torch.hstack(hu_list)\n",
    "        \n",
    "        scores = prediction_user_embedding @ self.wP(hi_list[0]).T\n",
    "        predictions = torch.softmax(scores, 0)\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "          \n",
    "\"\"\"\n",
    "Make network\n",
    "\"\"\"\n",
    "user_num = len(df['u'].unique())\n",
    "item_num = len(df['i'].unique())\n",
    "\n",
    "hidden_size = 64\n",
    "\n",
    "\n",
    "network = DGSRNetwork(user_num, item_num, hidden_size, user_max=n, item_max=n, num_DGRN_layers=1)\n",
    "\n",
    "\"\"\"\n",
    "Forward that shit\n",
    "\"\"\"\n",
    "graph = graphs[0]\n",
    "out = network(graph)\n",
    "\n",
    "out, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590c8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156f5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f42975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
